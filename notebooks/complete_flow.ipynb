{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import pyaudio\n",
    "import wave\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from collections import deque\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'vap_sound/src')))\n",
    "\n",
    "\n",
    "from utils.evaluate_model import evaluate_model\n",
    "from utils.extract_mfcc_torchaudio import extract_mfcc\n",
    "from utils.save_load_model import save_model_pickle, load_model_pickle\n",
    "\n",
    "from config import (\n",
    "    MFCC_SAMPLE_RATE, \n",
    "    N_MFCC_PER_FRAME,\n",
    "    MFCC_DIM,\n",
    "    SEQ_LENGTH,\n",
    "    NUM_CLASSES,\n",
    "    BATCH_SIZE,\n",
    "    NUM_EPOCHS,\n",
    "    LEARNING_RATE\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Variables\n",
    "\n",
    "MODEL_PATH = \"../model/vapor_model.pkl\"\n",
    "AUDIO_FILES_DIR = \"/Users/shanujha/Desktop/voice_activity_prediction/voice_data_mozilla/en/clips/\"\n",
    "CSV_FILES_DIR = \"/Users/shanujha/Desktop/voice_activity_prediction/mfcc_extract_csv/*.csv\"\n",
    "LOG_PROCESSED_FILES = \"../logs/processed_files.log\"\n",
    "LOG_MODEL_TRAINING = \"../logs/model_training.log\"\n",
    "LOG_MODEL_EVALUATION = \"../logs/model_evaluation.log\"\n",
    "LOG_MODEL_PREDICTION = \"../logs/model_prediction.log\"\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class VAPModel(nn.Module):\n",
    "#     def __init__(self, input_dim=N_MFCC_PER_FRAME, hidden_dim=256, num_heads=8, num_layers=6, output_dim=2):\n",
    "#         super(VAPModel, self).__init__()\n",
    "#         self.transformer = nn.TransformerEncoder(\n",
    "#             nn.TransformerEncoderLayer(d_model=input_dim, nhead=num_heads),\n",
    "#             num_layers=num_layers\n",
    "#         )\n",
    "#         self.fc = nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x = self.transformer(x)\n",
    "#         x = self.fc(x)\n",
    "#         return torch.sigmoid(x)\n",
    "    \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the VAP Model with LSTM\n",
    "class VAPModel(nn.Module):\n",
    "    def __init__(self, input_dim=N_MFCC_PER_FRAME, hidden_dim=128, lstm_hidden_dim=256, num_heads=8, num_layers=4, output_dim=1):\n",
    "        super(VAPModel, self).__init__()\n",
    "        \n",
    "        # Transformer layer\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=input_dim, nhead=num_heads),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        # LSTM Layer (make sure it's properly initialized)\n",
    "        self.lstm = nn.LSTM(input_dim, lstm_hidden_dim, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer to produce final output\n",
    "        self.fc = nn.Linear(lstm_hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Apply transformer for better feature encoding\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        # Apply LSTM for sequence modeling\n",
    "        x, _ = self.lstm(x)\n",
    "        \n",
    "        # Apply fully connected layer\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return torch.sigmoid(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class for extracting MFCC features and labels\n",
    "class VAPDataset(Dataset):\n",
    "    def __init__(self, csv_files, seq_length=SEQ_LENGTH):\n",
    "        if isinstance(csv_files, str):\n",
    "            csv_files = [csv_files]\n",
    "        \n",
    "        self.seq_length = seq_length\n",
    "        self.data = pd.concat([pd.read_csv(f) for f in csv_files], ignore_index=True)\n",
    "        self.features = [torch.tensor(eval(f)) for f in self.data['features']]\n",
    "        self.labels = [torch.tensor(eval(l))[:,0] for l in self.data['labels']]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.features[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if feature.shape[0] > self.seq_length:\n",
    "            feature = feature[-self.seq_length:]\n",
    "            label = label[-self.seq_length:]\n",
    "        else:\n",
    "            pad_length = self.seq_length - feature.shape[0]\n",
    "            feature = torch.cat([torch.zeros(pad_length, feature.shape[1]), feature], dim=0)\n",
    "            label = torch.cat([torch.zeros(pad_length, label.shape[1]), label], dim=0)\n",
    "        \n",
    "        return feature, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from a folder containing MP3 files and save to CSV\n",
    "data_dir = AUDIO_FILES_DIR\n",
    "audio_files = glob(os.path.join(data_dir, \"*.mp3\"))\n",
    "csv_files = glob(CSV_FILES_DIR)\n",
    "\n",
    "# Splitting dataset into training and testing\n",
    "train_files, test_files = train_test_split(csv_files, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate function to pad variable-length sequences\n",
    "def collate_fn(batch):\n",
    "    features, labels = zip(*batch)\n",
    "    features_padded = pad_sequence(features, batch_first=True, padding_value=0)\n",
    "    labels_padded = pad_sequence(labels, batch_first=True, padding_value=0)\n",
    "    return features_padded, labels_padded\n",
    "\n",
    "# Function to track processed files\n",
    "def get_processed_files(log_file=LOG_PROCESSED_FILES):\n",
    "    if os.path.exists(log_file):\n",
    "        with open(log_file, \"r\") as f:\n",
    "            return set(f.read().splitlines())\n",
    "    return set()\n",
    "\n",
    "def update_processed_files(files, log_file=LOG_PROCESSED_FILES):\n",
    "    with open(log_file, \"a\") as f:\n",
    "        for file in files:\n",
    "            f.write(file + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 61\u001b[0m\n\u001b[1;32m     58\u001b[0m             evaluate_model(model, eval_loader, output_file\u001b[38;5;241m=\u001b[39mLOG_MODEL_EVALUATION, metrics_file\u001b[38;5;241m=\u001b[39mLOG_MODEL_PREDICTION)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Train the model in batches\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mVAPModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSEQ_LENGTH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[96], line 49\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, csv_files, batch_size, epochs, log_file, processed_log, seq_length)\u001b[0m\n\u001b[1;32m     47\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(features)\n\u001b[1;32m     48\u001b[0m outputs \u001b[38;5;241m=\u001b[39m outputs[:, :min_length, :]\n\u001b[0;32m---> 49\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mmin_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     50\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     51\u001b[0m val_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training loop with batch-wise dataset loading\n",
    "def train_model(model, csv_files, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, log_file=LOG_MODEL_TRAINING, processed_log=LOG_PROCESSED_FILES, seq_length=SEQ_LENGTH):\n",
    "    processed_files = get_processed_files(processed_log)\n",
    "    remaining_files = [f for f in csv_files if f not in processed_files]\n",
    "    total_files = len(remaining_files)\n",
    "    \n",
    "    with open(log_file, \"a\") as log:\n",
    "        for i in range(0, total_files, batch_size):\n",
    "            batch_files = remaining_files[i:i+batch_size]\n",
    "            train_dataset = VAPDataset_m(batch_files, seq_length=seq_length)\n",
    "            # print(type(train_dataset), train_dataset)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "            # print(type(train_loader), train_loader)\n",
    "\n",
    "            eval_files = np.random.choice(test_files, 5)\n",
    "            eval_dataset = VAPDataset_m(eval_files, seq_length=seq_length)\n",
    "            eval_loader = DataLoader(eval_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
    "            \n",
    "            if os.path.exists(MODEL_PATH):\n",
    "                model = load_model_pickle(path=MODEL_PATH)\n",
    "            \n",
    "            optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "            criterion = nn.BCELoss()\n",
    "            \n",
    "            for epoch in range(epochs):\n",
    "                model.train()\n",
    "                total_loss = 0\n",
    "                val_loss = 0\n",
    "                \n",
    "                for features, labels in train_loader:\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(features)\n",
    "                    \n",
    "                    min_length = min(outputs.shape[1], labels.shape[1])\n",
    "                    outputs = outputs[:, :min_length, :]\n",
    "                    # labels = labels[:, :min_length, :]\n",
    "                    labels = labels[:, :min_length].unsqueeze(-1)\n",
    "                    \n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    total_loss += loss.item()\n",
    "                \n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    for features, labels in train_loader:\n",
    "                        outputs = model(features)\n",
    "                        outputs = outputs[:, :min_length, :]\n",
    "                        labels = labels[:, :min_length, :]\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        val_loss += loss.item()\n",
    "                \n",
    "                log.write(f\"Batch {i//batch_size+1}/{total_files//batch_size+1}, Epoch {epoch+1}/{epochs}, Train Loss: {total_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(train_loader):.4f}\\n\")\n",
    "                print(f\"Batch {i//batch_size+1}/{total_files//batch_size+1}, Epoch {epoch+1}/{epochs}, Train Loss: {total_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(train_loader):.4f}\")\n",
    "            \n",
    "            save_model_pickle(model, path=MODEL_PATH)\n",
    "            update_processed_files(batch_files, processed_log)\n",
    "            evaluate_model(model, eval_loader, output_file=LOG_MODEL_EVALUATION, metrics_file=LOG_MODEL_PREDICTION)\n",
    "\n",
    "# Train the model in batches\n",
    "train_model(VAPModel(), train_files, batch_size=10, epochs=10, seq_length=SEQ_LENGTH)\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAPDataset_m(Dataset):\n",
    "    def __init__(self, csv_files, seq_length=SEQ_LENGTH):\n",
    "        if isinstance(csv_files, str):\n",
    "            csv_files = [csv_files]\n",
    "\n",
    "        self.seq_length = seq_length\n",
    "        self.data = pd.concat([pd.read_csv(f) for f in csv_files], ignore_index=True)\n",
    "        self.features = [torch.tensor(eval(f)) for f in self.data['features']]\n",
    "        \n",
    "        # Extract only the first label\n",
    "        lb = [eval(f)[0] for f in self.data['labels']]\n",
    "        self.labels = torch.tensor(lb).float()  # Ensure it's a float tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.features[idx]\n",
    "        label = self.labels[idx]  # Scalar value\n",
    "\n",
    "        # Trim or pad feature sequence\n",
    "        if feature.shape[0] > self.seq_length:\n",
    "            feature = feature[-self.seq_length:]\n",
    "        else:\n",
    "            pad_length = self.seq_length - feature.shape[0]\n",
    "            feature = torch.cat([torch.zeros(pad_length, feature.shape[1]), feature], dim=0)\n",
    "\n",
    "        return feature, label.unsqueeze(0)  # Ensure label is a tensor of shape (1,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = VAPDataset_m([\"/Users/shanujha/Desktop/voice_activity_prediction/mfcc_extract_csv/dataset_1.csv\"], seq_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [[0.0, 0.0], [0.0, 0.0], [1.0, 1.0], [1.0, 1.0...\n",
       "1     [[0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0...\n",
       "2     [[1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [0.0, 0.0...\n",
       "3     [[1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0...\n",
       "4     [[0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0...\n",
       "                            ...                        \n",
       "92    [[0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0...\n",
       "93    [[0.0, 0.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0...\n",
       "94    [[0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0...\n",
       "95    [[1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [0.0, 0.0...\n",
       "96    [[0.0, 0.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0...\n",
       "Name: labels, Length: 97, dtype: object"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[-337.7912902832031, 1.5555504432995804e-05, ...</td>\n",
       "      <td>[[0.0, 0.0], [0.0, 0.0], [1.0, 1.0], [1.0, 1.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[-359.0429992675781, -7.660827350264299e-08, ...</td>\n",
       "      <td>[[0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[-308.6006774902344, 1.2662911103689112e-05, ...</td>\n",
       "      <td>[[1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [0.0, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[-318.2252197265625, 2.025314825004898e-05, -...</td>\n",
       "      <td>[[1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[-296.0960693359375, 2.2307773178908974e-05, ...</td>\n",
       "      <td>[[0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>[[-327.2367248535156, 1.7432679669582285e-05, ...</td>\n",
       "      <td>[[0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>[[-328.3695068359375, 6.071793450246332e-06, -...</td>\n",
       "      <td>[[0.0, 0.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>[[-378.6844177246094, -1.5926906371532823e-06,...</td>\n",
       "      <td>[[0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>[[-292.9357604980469, 1.2051293651893502e-07, ...</td>\n",
       "      <td>[[1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [0.0, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>[[-313.1432800292969, 7.065336831146851e-06, -...</td>\n",
       "      <td>[[0.0, 0.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>97 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             features  \\\n",
       "0   [[-337.7912902832031, 1.5555504432995804e-05, ...   \n",
       "1   [[-359.0429992675781, -7.660827350264299e-08, ...   \n",
       "2   [[-308.6006774902344, 1.2662911103689112e-05, ...   \n",
       "3   [[-318.2252197265625, 2.025314825004898e-05, -...   \n",
       "4   [[-296.0960693359375, 2.2307773178908974e-05, ...   \n",
       "..                                                ...   \n",
       "92  [[-327.2367248535156, 1.7432679669582285e-05, ...   \n",
       "93  [[-328.3695068359375, 6.071793450246332e-06, -...   \n",
       "94  [[-378.6844177246094, -1.5926906371532823e-06,...   \n",
       "95  [[-292.9357604980469, 1.2051293651893502e-07, ...   \n",
       "96  [[-313.1432800292969, 7.065336831146851e-06, -...   \n",
       "\n",
       "                                               labels  \n",
       "0   [[0.0, 0.0], [0.0, 0.0], [1.0, 1.0], [1.0, 1.0...  \n",
       "1   [[0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0...  \n",
       "2   [[1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [0.0, 0.0...  \n",
       "3   [[1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0...  \n",
       "4   [[0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0...  \n",
       "..                                                ...  \n",
       "92  [[0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0...  \n",
       "93  [[0.0, 0.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0...  \n",
       "94  [[0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0...  \n",
       "95  [[1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [0.0, 0.0...  \n",
       "96  [[0.0, 0.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0...  \n",
       "\n",
       "[97 rows x 2 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader.dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/Users/shanujha/Desktop/voice_activity_prediction/mfcc_extract_csv/dataset_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "b = eval(data['labels'][0])\n",
    "a = []\n",
    "\n",
    "for i in range(len(b)):\n",
    "    print(b[i][0])\n",
    "    a.append(b[i][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
       "        0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "        1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
       "        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0.,\n",
       "        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
