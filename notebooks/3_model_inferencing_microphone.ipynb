{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load model\n",
    "import pickle\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'vap_sound/src')))\n",
    "\n",
    "from vap_model import VAPModel\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Function to load model from pickle file\n",
    "def load_model_pickle(path=os.environ.get(\"MODEL_PATH\")):\n",
    "    with open(path, \"rb\") as f:\n",
    "        model = pickle.load(f)\n",
    "    model.eval()\n",
    "    print(\"Model loaded successfully from pickle file!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import torchaudio.transforms as T\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Real-time inference from microphone\n",
    "def infer_from_mic(model, sample_rate=16000, chunk_size=1024):\n",
    "    p = pyaudio.PyAudio()\n",
    "    stream = p.open(format=pyaudio.paInt16, channels=1, rate=sample_rate, input=True, frames_per_buffer=chunk_size)\n",
    "    mfcc_transform = T.MFCC(sample_rate=sample_rate, n_mfcc=40, \n",
    "                            melkwargs={\"n_fft\": 400, \"hop_length\": 160, \"n_mels\": 40})\n",
    "    print(\"Listening...\")\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            audio_data = stream.read(chunk_size)\n",
    "            waveform = torch.from_numpy(np.frombuffer(audio_data, dtype=np.int16).astype(np.float32) / 32768.0).unsqueeze(0)\n",
    "            mfcc = mfcc_transform(waveform).squeeze(0).T.unsqueeze(0)  # Add batch dimension\n",
    "            with torch.no_grad():\n",
    "                output = model(mfcc)\n",
    "                prediction = output.squeeze(0).cpu().numpy()\n",
    "                print(\"Prediction:\", prediction)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Stopping...\")\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "        p.terminate()\n",
    "\n",
    "# # Load trained model and run inference\n",
    "# model = load_model_pickle()\n",
    "# infer_from_mic(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import torch\n",
    "import torchaudio.transforms as T\n",
    "import numpy as np\n",
    "\n",
    "# Real-time inference from microphone, streaming to a function\n",
    "def infer_from_mic_stream(model, callback_function, sample_rate=16000, chunk_size=1024):\n",
    "# def infer_from_mic_stream(model, sample_rate=16000, chunk_size=1024):\n",
    "    \"\"\"\n",
    "    Streams microphone audio to a callback function for real-time inference.\n",
    "\n",
    "    Args:\n",
    "        model: The PyTorch model for inference.\n",
    "        callback_function: A function that will receive the model's prediction.\n",
    "        sample_rate: The audio sample rate.\n",
    "        chunk_size: The size of audio chunks to process.\n",
    "    \"\"\"\n",
    "    p = pyaudio.PyAudio()\n",
    "    stream = p.open(format=pyaudio.paInt16, channels=1, rate=sample_rate, input=True, frames_per_buffer=chunk_size)\n",
    "    mfcc_transform = T.MFCC(sample_rate=sample_rate, n_mfcc=40,\n",
    "                            melkwargs={\"n_fft\": 400, \"hop_length\": 160, \"n_mels\": 40})\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            audio_data = stream.read(chunk_size)\n",
    "            waveform = torch.from_numpy(np.frombuffer(audio_data, dtype=np.int16).astype(np.float32) / 32768.0).unsqueeze(0)\n",
    "            mfcc = mfcc_transform(waveform).squeeze(0).T.unsqueeze(0)  # Add batch dimension\n",
    "            with torch.no_grad():\n",
    "                output = model(mfcc)\n",
    "                prediction = output.squeeze(0).cpu().numpy()\n",
    "                array = np.array(prediction)\n",
    "                mean = np.mean(array)\n",
    "                avg = mean.item()\n",
    "                # print(avg)\n",
    "                # print(prediction)\n",
    "                callback_function(avg)  # Call the provided function with the prediction\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "        p.terminate()\n",
    "\n",
    "\n",
    "# infer_from_mic_stream(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from pickle file!\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import numpy as np\n",
    "import time\n",
    "import threading\n",
    "\n",
    "# Initialize lists to store the data points\n",
    "max_window_seconds = 20  # Sliding window of 20 seconds\n",
    "data = []\n",
    "timestamps = []\n",
    "\n",
    "# # Lock for thread-safe access to shared data\n",
    "data_lock = threading.Lock()\n",
    "\n",
    "# type(data_lock)\n",
    "\n",
    "def process_pred(prediction):\n",
    "    # Append the scalar value to the data list\n",
    "    current_time = time.time()\n",
    "    with data_lock:  # Ensure thread-safe access to shared data\n",
    "        data.append(prediction)  # Use the first element of column_average\n",
    "        timestamps.append(current_time)\n",
    "        \n",
    "        # Remove data points older than the sliding window\n",
    "        while timestamps and (current_time - timestamps[0]) > max_window_seconds:\n",
    "            timestamps.pop(0)\n",
    "            data.pop(0)\n",
    "    \n",
    "    # Print for debugging\n",
    "    print(\"Data: \", data)\n",
    "    print(\"Timestamps: \", timestamps)\n",
    "\n",
    "\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "LOG_FILE = os.environ.get(\"MODEL_PREDICTION_FILE\")\n",
    "\n",
    "def write_to_file_pred(prediction):\n",
    "    log_file = LOG_FILE\n",
    "    \n",
    "    # Create the log file if it doesn't exist\n",
    "    if not os.path.exists(log_file):\n",
    "        with open(log_file, \"w\") as f:\n",
    "            f.write(\"PREDICTION LOG\\n\")  # Add a header\n",
    "\n",
    "    # Append new prediction with timestamp\n",
    "    # cur_time = time.strftime(\"%Y-%m-%d %H:%M:%S\")  # Readable timestamp\n",
    "    cur_time = time.time()\n",
    "    with open(log_file, \"a\") as f:  # Append mode\n",
    "        f.write(f\"{cur_time} Prediction: {prediction}\\n\")\n",
    "\n",
    "\n",
    "def remove_log_file():\n",
    "    \"\"\"Removes the log file after execution.\"\"\"\n",
    "    if os.path.exists(LOG_FILE):\n",
    "        os.remove(LOG_FILE)\n",
    "        print(f\"Deleted log file: {LOG_FILE}\")\n",
    "\n",
    "\n",
    "\n",
    "model = load_model_pickle()\n",
    "infer_from_mic_stream(model=model, callback_function=write_to_file_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted log file: ../pred.txt\n"
     ]
    }
   ],
   "source": [
    "remove_log_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
